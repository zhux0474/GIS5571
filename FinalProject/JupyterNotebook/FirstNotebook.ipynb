{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project GEOG5543 Part 1 Extract Interaction Data\n",
    "The machine learning method k means clustering algorithm and density-based spatial clustering of applications with noise (DBSCAN) method will be used to group sets of latitude and longitude coordinate points from Travel Behavior Inventory (TBI) Household Survey Interview Data for 2010 in Minnesota to determine the clustering pattern of individual interaction (trip destination) location. The interaction between survey participants is defined by the time geography concept and space-time coupling constraints. The latitude and longitude coordinate points will be grouped into k groups using the unsupervised k means algorithm and it will be tested in DBSCAN to compare the results of these two different methods. This project will combine spatial analysis with the power of machine learning to determine and obtain the clustering pattern of TBI data to understand how the trip destination locations are distributed spatially and to test the performance of two different clustering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Potential Interaction\n",
    "The potential interaction is defined and calculated between two persons using space-time coupling constraints, that is, if and only if two persons are present at the same location during the same time can they potentially interact. An individual is exposed to interaction with others in each location he or she visits. If person A and person B spend time at two different locations, there is no potential interaction between them. The interaction starts when they are presented at the same location at the same time. The interaction stops once either one of them exit the location. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Interaction Data\n",
    "In this notebook, Python is used to clean and process the merged table from TBI data. Python code is used to ignore the null value and missing data and then to identify participants that were presented at the same location at the same period of time and extract their activity information into new tables. \n",
    "\n",
    "\n",
    "The data is first sorted by ID, start time and end time of the trip so that they are listed as consecutive trips that are recorded from the same person. And the start time and end time of each trip are compared to identified two persons that are presented at the same location during the same time so if there is overlapping time period spend at the same location between two individuals, there is potential interaction between them based on the definition. The GPS locations, day of the week, person ID is extracted and further separated into five days of the week days, that is, Monday, Tuesday, Wednesday, Thursday, and Friday.\n",
    "\n",
    "After the latitude and longitude data is ready in CSV format, the k means and DBSCAN will be applied to the data and generate a clustering point map in Folium map with dropdown widget in Part 2(second notebook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import csv\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (20,26,28,51,52,57,127,128,190,193,199) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "#Read in the merged data\n",
    "df = pd.read_csv('Merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the trip record and insert activities between two consecutive trips; \n",
    "1. Ignore when trip origin and trip destination spatial unit is missing or null\n",
    "2. Ignore when activity origin and destination spatial units is missing or null\n",
    "3. Use the destination description of the previous trip to describe the following activity place\n",
    "4. Identify all the person B that person A interact with because person A might interact with more than one person B at different times on different days \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract selected column into new datafram df3\n",
    "df3 = df[['HHPERSONID', 'TRIPID', 'HHID', 'HOMELAT','HOMELON','HTAZ',\n",
    "                 'TRVDATE','DAY','OLAT','OLON','OTAZ', 'OLOCTYPE','OLOCACT1','LVLOCTIME','TRANSTYPE1',\n",
    "                 'ARVTIME', 'DLAT','DLON','DTAZ','DLOCTYPE','DLOCACT1' , \n",
    "                 'origAct','destAct','mode1', 'arvtime_fin','lvloctime_fin',\n",
    "                 'Gender','Student','Educ','Wrkr','Age3'\n",
    "                 ]]\n",
    "\n",
    "#convert start/end date and time into string with same format\n",
    "df3['TRVDATE']= df3['TRVDATE'].astype('string')#Date\n",
    "df3['lvloctime_fin'] = df3['lvloctime_fin'].astype('string')#start time\n",
    "df3['lvloctime_fin'] = df3['lvloctime_fin'].str.zfill(4)#start time\n",
    "df3['ARVTIME']=df3['ARVTIME'].astype('string')#end time\n",
    "df3['ARVTIME'] = df3['ARVTIME'].str.zfill(4)#end time\n",
    "\n",
    "#Create new variables to store Date + Time\n",
    "df3['Start_DateTime'] = df3['TRVDATE']+ df3['lvloctime_fin']\n",
    "df3['End_DateTime'] = df3['TRVDATE']+ df3['ARVTIME']\n",
    "df3['Start_Time'] = pd.to_datetime(df3['Start_DateTime'],format ='%Y%m%d%H%M')#start time\n",
    "df3['End_Time'] = pd.to_datetime(df3['End_DateTime'],format ='%Y%m%d%H%M')#end time\n",
    "\n",
    "#Convert HHPERSONID into string\n",
    "df3['HHPERSONID'] = df3['HHPERSONID'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HHPERSONID</th>\n",
       "      <th>TRIPID</th>\n",
       "      <th>HHID</th>\n",
       "      <th>HOMELAT</th>\n",
       "      <th>HOMELON</th>\n",
       "      <th>HTAZ</th>\n",
       "      <th>TRVDATE</th>\n",
       "      <th>DAY</th>\n",
       "      <th>OLAT</th>\n",
       "      <th>OLON</th>\n",
       "      <th>...</th>\n",
       "      <th>lvloctime_fin</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Student</th>\n",
       "      <th>Educ</th>\n",
       "      <th>Wrkr</th>\n",
       "      <th>Age3</th>\n",
       "      <th>Start_DateTime</th>\n",
       "      <th>End_DateTime</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>End_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10002101</td>\n",
       "      <td>1000210101</td>\n",
       "      <td>100021</td>\n",
       "      <td>44.954</td>\n",
       "      <td>-93.337</td>\n",
       "      <td>1374</td>\n",
       "      <td>20110610</td>\n",
       "      <td>6</td>\n",
       "      <td>44.954</td>\n",
       "      <td>-93.337</td>\n",
       "      <td>...</td>\n",
       "      <td>0830</td>\n",
       "      <td>Male</td>\n",
       "      <td>Not in school</td>\n",
       "      <td>Graduate/Post-graduate degree</td>\n",
       "      <td>A PAID part-time worker</td>\n",
       "      <td>7</td>\n",
       "      <td>201106100830</td>\n",
       "      <td>201106100945</td>\n",
       "      <td>2011-06-10 08:30:00</td>\n",
       "      <td>2011-06-10 09:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002101</td>\n",
       "      <td>1000210102</td>\n",
       "      <td>100021</td>\n",
       "      <td>44.954</td>\n",
       "      <td>-93.337</td>\n",
       "      <td>1374</td>\n",
       "      <td>20110610</td>\n",
       "      <td>6</td>\n",
       "      <td>44.954</td>\n",
       "      <td>-93.337</td>\n",
       "      <td>...</td>\n",
       "      <td>1030</td>\n",
       "      <td>Male</td>\n",
       "      <td>Not in school</td>\n",
       "      <td>Graduate/Post-graduate degree</td>\n",
       "      <td>A PAID part-time worker</td>\n",
       "      <td>7</td>\n",
       "      <td>201106101030</td>\n",
       "      <td>201106101035</td>\n",
       "      <td>2011-06-10 10:30:00</td>\n",
       "      <td>2011-06-10 10:35:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002101</td>\n",
       "      <td>1000210103</td>\n",
       "      <td>100021</td>\n",
       "      <td>44.954</td>\n",
       "      <td>-93.337</td>\n",
       "      <td>1374</td>\n",
       "      <td>20110610</td>\n",
       "      <td>6</td>\n",
       "      <td>44.954</td>\n",
       "      <td>-93.337</td>\n",
       "      <td>...</td>\n",
       "      <td>1040</td>\n",
       "      <td>Male</td>\n",
       "      <td>Not in school</td>\n",
       "      <td>Graduate/Post-graduate degree</td>\n",
       "      <td>A PAID part-time worker</td>\n",
       "      <td>7</td>\n",
       "      <td>201106101040</td>\n",
       "      <td>201106101045</td>\n",
       "      <td>2011-06-10 10:40:00</td>\n",
       "      <td>2011-06-10 10:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10002101</td>\n",
       "      <td>1000210104</td>\n",
       "      <td>100021</td>\n",
       "      <td>44.954</td>\n",
       "      <td>-93.337</td>\n",
       "      <td>1374</td>\n",
       "      <td>20110610</td>\n",
       "      <td>6</td>\n",
       "      <td>44.954</td>\n",
       "      <td>-93.337</td>\n",
       "      <td>...</td>\n",
       "      <td>1145</td>\n",
       "      <td>Male</td>\n",
       "      <td>Not in school</td>\n",
       "      <td>Graduate/Post-graduate degree</td>\n",
       "      <td>A PAID part-time worker</td>\n",
       "      <td>7</td>\n",
       "      <td>201106101145</td>\n",
       "      <td>201106101155</td>\n",
       "      <td>2011-06-10 11:45:00</td>\n",
       "      <td>2011-06-10 11:55:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10002101</td>\n",
       "      <td>1000210105</td>\n",
       "      <td>100021</td>\n",
       "      <td>44.954</td>\n",
       "      <td>-93.337</td>\n",
       "      <td>1374</td>\n",
       "      <td>20110610</td>\n",
       "      <td>6</td>\n",
       "      <td>44.968</td>\n",
       "      <td>-93.354</td>\n",
       "      <td>...</td>\n",
       "      <td>1320</td>\n",
       "      <td>Male</td>\n",
       "      <td>Not in school</td>\n",
       "      <td>Graduate/Post-graduate degree</td>\n",
       "      <td>A PAID part-time worker</td>\n",
       "      <td>7</td>\n",
       "      <td>201106101320</td>\n",
       "      <td>201106101330</td>\n",
       "      <td>2011-06-10 13:20:00</td>\n",
       "      <td>2011-06-10 13:30:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  HHPERSONID      TRIPID    HHID  HOMELAT  HOMELON  HTAZ   TRVDATE  DAY  \\\n",
       "0   10002101  1000210101  100021   44.954  -93.337  1374  20110610    6   \n",
       "1   10002101  1000210102  100021   44.954  -93.337  1374  20110610    6   \n",
       "2   10002101  1000210103  100021   44.954  -93.337  1374  20110610    6   \n",
       "3   10002101  1000210104  100021   44.954  -93.337  1374  20110610    6   \n",
       "4   10002101  1000210105  100021   44.954  -93.337  1374  20110610    6   \n",
       "\n",
       "     OLAT    OLON  ...  lvloctime_fin Gender        Student  \\\n",
       "0  44.954 -93.337  ...           0830   Male  Not in school   \n",
       "1  44.954 -93.337  ...           1030   Male  Not in school   \n",
       "2  44.954 -93.337  ...           1040   Male  Not in school   \n",
       "3  44.954 -93.337  ...           1145   Male  Not in school   \n",
       "4  44.968 -93.354  ...           1320   Male  Not in school   \n",
       "\n",
       "                            Educ                     Wrkr Age3  \\\n",
       "0  Graduate/Post-graduate degree  A PAID part-time worker    7   \n",
       "1  Graduate/Post-graduate degree  A PAID part-time worker    7   \n",
       "2  Graduate/Post-graduate degree  A PAID part-time worker    7   \n",
       "3  Graduate/Post-graduate degree  A PAID part-time worker    7   \n",
       "4  Graduate/Post-graduate degree  A PAID part-time worker    7   \n",
       "\n",
       "   Start_DateTime  End_DateTime          Start_Time            End_Time  \n",
       "0    201106100830  201106100945 2011-06-10 08:30:00 2011-06-10 09:45:00  \n",
       "1    201106101030  201106101035 2011-06-10 10:30:00 2011-06-10 10:35:00  \n",
       "2    201106101040  201106101045 2011-06-10 10:40:00 2011-06-10 10:45:00  \n",
       "3    201106101145  201106101155 2011-06-10 11:45:00 2011-06-10 11:55:00  \n",
       "4    201106101320  201106101330 2011-06-10 13:20:00 2011-06-10 13:30:00  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Finish processing\n"
     ]
    }
   ],
   "source": [
    "# create a new column to store the activity sequences for each person\n",
    "\n",
    "#Create an activity id \n",
    "df3['ACTIVITY_ID'] = np.nan\n",
    "\n",
    "# sort the record by HHPERSONID and by Start Time and End Time\n",
    "df3 = df3.sort_values(['HHPERSONID', 'Start_Time', 'End_Time'],ascending=[True, True, True])\n",
    "\n",
    "# Group the data by 'HHPERSONID'\n",
    "grouped = df3.groupby('HHPERSONID')\n",
    "\n",
    "# create an empty dictionary for output\n",
    "new_dict = {'HHPERSONID':[], 'TRIPID':[], 'ACTIVITY_ID':[], \n",
    "            'Start_Time':[], 'End_Time':[],\n",
    "            'DAY':[], 'OLAT':[],\n",
    "            'OLON':[], 'OTAZ':[],'DLAT':[],\n",
    "            'DLON':[], 'DTAZ':[],\n",
    "            'DLOCTYPE':[],'DLOCACT1':[],\n",
    "           'Gender':[],'Student':[],'Educ':[],'Wrkr':[],'Age3':[]}\n",
    "\n",
    "testCnt = 1\n",
    "count =1\n",
    "k = 0\n",
    "\n",
    "#processing each person's record using for loop to loop through the grouped value\n",
    "for name, group in grouped:\n",
    "    k += 1\n",
    "\n",
    "    totalCount = group.shape[0]# get the total number of records for that person\n",
    "    activityCount = 0# get the total number of activity for that person to create TRIP_ID\n",
    "    \n",
    "    # insert the first activity episode; keep Person ID, update datetime and TAZ, keep others variables as nan\n",
    "    act_row = group.iloc[0]\n",
    "    new_dict['HHPERSONID'].append(act_row['HHPERSONID'])\n",
    "    new_dict['TRIPID'].append(np.nan)\n",
    "    new_dict['ACTIVITY_ID'].append(np.nan)\n",
    "    tempDateTime = pd.to_datetime(act_row['Start_Time'].strftime('%Y-%m-%d') + ' 00:00:00')\n",
    "    new_dict['Start_Time'].append(tempDateTime)\n",
    "    new_dict['End_Time'].append(act_row['Start_Time'])\n",
    "    new_dict['DLAT'].append(np.nan)\n",
    "    new_dict['DLON'].append(np.nan)\n",
    "    new_dict['OTAZ'].append(np.nan)\n",
    "    new_dict['DTAZ'].append(act_row['OTAZ'])\n",
    "    new_dict['Gender'].append(np.nan)\n",
    "    new_dict['Student'].append(np.nan)\n",
    "    new_dict['Educ'].append(np.nan)\n",
    "    new_dict['Wrkr'].append(np.nan)\n",
    "    new_dict['Age3'].append(np.nan)\n",
    "    \n",
    "    prev_row = group.iloc[0]\n",
    "    for key in new_dict.keys():#add to dictionary\n",
    "        new_dict[key].append(prev_row[key])\n",
    "    next_row = None\n",
    "    \n",
    "    i = 1\n",
    "    \n",
    "    # if i is less than the total number of records, iterately update the dictionary\n",
    "    while i < totalCount:\n",
    "        next_row = group.iloc[i]\n",
    "        prev_row = group.iloc[i-1]\n",
    "        new_dict['HHPERSONID'].append(next_row['HHPERSONID'])\n",
    "        new_dict['TRIPID'].append(np.nan)\n",
    "        new_dict['Start_Time'].append(prev_row['End_Time'])\n",
    "        new_dict['End_Time'].append(next_row['Start_Time'])\n",
    "        new_dict['DLAT'].append(prev_row['DLAT'])\n",
    "        new_dict['DLON'].append(prev_row['DLON'])\n",
    "        new_dict['OTAZ'].append(prev_row['OTAZ'])\n",
    "        new_dict['DTAZ'].append(next_row['DTAZ'])\n",
    "        new_dict['Gender'].append(next_row['Gender'])\n",
    "        new_dict['Student'].append(next_row['Student'])\n",
    "        new_dict['Educ'].append(next_row['Educ'])\n",
    "        new_dict['Wrkr'].append(next_row['Wrkr'])\n",
    "        new_dict['Age3'].append(next_row['Age3'])\n",
    "        # if the \"to time(end time)\" from previous row is less than the \"from time(start time)\" of the next row,(to keep it consecutive) \n",
    "        # update the activity ID by combining HHPERSONID with the number of activity of that person\n",
    "        #(if not, keep it as null)\n",
    "        if (prev_row['End_Time'] < next_row['Start_Time']):\n",
    "            activityCount += 1\n",
    "            cur_activityID = next_row['HHPERSONID'] + str(activityCount).zfill(3)\n",
    "            new_dict['ACTIVITY_ID'].append(cur_activityID)\n",
    "        else:\n",
    "            new_dict['ACTIVITY_ID'].append(np.nan)\n",
    "        \n",
    "        for key in new_dict.keys():\n",
    "            new_dict[key].append(next_row[key])\n",
    "        #update i\n",
    "        i += 1\n",
    "        \n",
    "    # update the rest of the record in the dictionary\n",
    "    if next_row is None:\n",
    "        # if next_row is none which means that there is only one record, set it as the previous\n",
    "        # else set it as the next row\n",
    "        act_row = prev_row\n",
    "    else:\n",
    "        act_row = next_row\n",
    "    \n",
    "    #append all the rows to the empty dictionary new_dict\n",
    "    new_dict['HHPERSONID'].append(act_row['HHPERSONID'])\n",
    "    new_dict['TRIPID'].append(np.nan)\n",
    "    new_dict['ACTIVITY_ID'].append(np.nan)\n",
    "    new_dict['Start_Time'].append(act_row['End_Time'])\n",
    "    tempDateTime = pd.to_datetime(act_row['End_Time'].strftime('%Y-%m-%d') + ' 23:59:00')\n",
    "    new_dict['End_Time'].append(tempDateTime)\n",
    "    new_dict['DLAT'].append(act_row['DLAT'])\n",
    "    new_dict['DLON'].append(act_row['DLAT'])\n",
    "    new_dict['OTAZ'].append(act_row['OTAZ'])\n",
    "    new_dict['DTAZ'].append(np.nan)\n",
    "    new_dict['Gender'].append(act_row['Gender'])\n",
    "    new_dict['Student'].append(act_row['Student'])\n",
    "    new_dict['Educ'].append(act_row['Educ'])\n",
    "    new_dict['Wrkr'].append(act_row['Wrkr'])\n",
    "    new_dict['Age3'].append(act_row['Age3'])\n",
    "print('>>> Finish processing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dictionary to dataframe and export it to csv file\n",
    "new_df=pd.DataFrame.from_dict(new_dict,orient='index').transpose()\n",
    "new_df=pd.DataFrame(dict([(k,pd.Series(v))for k,v in new_dict.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to CSV\n",
    "new_df.to_csv('Activity.csv', index=False, \n",
    "            columns=['HHPERSONID','TRIPID','ACTIVITY_ID','Start_Time','End_Time','DAY','OLAT','OLON','OTAZ','DLAT','DLON', 'DTAZ','DLOCTYPE','DLOCACT1','Gender','Student','Educ','Wrkr','Age3'\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find potential interactiion and break down trip records into five days (Monday to Friday)\n",
    "Monday 0, Tuesday 1, Wednesday 2, Thursday 3, Friday 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the start time and end time of each trip are compared to identified two persons that are presented at the same location during the same time so if there is overlapping time period spend at the same location between two individuals, there is potential interaction between them based on the definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Read in the csv file that is generated in above cell\n",
    "df4 = pd.read_csv(\"Activity.csv\")\n",
    "\n",
    "# change the data type of the datetime\n",
    "dt_format = '%Y-%m-%d %H:%M:%S'\n",
    "df4['Start_Time'] = pd.to_datetime(df4['Start_Time'], format = dt_format)\n",
    "df4['End_Time'] = pd.to_datetime(df4['End_Time'], format = dt_format)\n",
    "\n",
    "# sort the data \n",
    "df4 = df4.sort_values(['HHPERSONID', 'Start_Time', 'End_Time'],ascending=[True, True, True])\n",
    "\n",
    "# day of the week, with Monday=0, Tuesday 1, Wednesday 2, Thursday 3, Friday 4\n",
    "df4['day_of_week'] = df4['Start_Time'].dt.dayofweek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# select only the activities that occured in the same TAZ and format columns\n",
    "df4 = df4[~df4['ACTIVITY_ID'].isnull()]\n",
    "df4 = df4.query('OTAZ == DTAZ')\n",
    "df4['Start_Time'] = pd.to_datetime(df4['Start_Time'])\n",
    "df4['End_Time'] = pd.to_datetime(df4['End_Time'])\n",
    "df4['day_of_week'] =df4['day_of_week'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Now start to process weekday: 0\n",
      ">>> Finish writing the output data Interaction_ByDOW_0.csv\n",
      ">>> Now start to process weekday: 1\n",
      ">>> Finish writing the output data Interaction_ByDOW_1.csv\n",
      ">>> Now start to process weekday: 2\n",
      ">>> Finish writing the output data Interaction_ByDOW_2.csv\n",
      ">>> Now start to process weekday: 3\n",
      ">>> Finish writing the output data Interaction_ByDOW_3.csv\n",
      ">>> Now start to process weekday: 4\n",
      ">>> Finish writing the output data Interaction_ByDOW_4.csv\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# group by day of the week\n",
    "DOW_grouped = df4.groupby('day_of_week')\n",
    "testCnt = 0\n",
    "tempDate = dt.date.today()\n",
    "\n",
    "\n",
    "#for all the day of the week (monday to friday in this case)\n",
    "#processing each record using for loop to loop through the grouped value\n",
    "for DOW, DOW_group in DOW_grouped:\n",
    "    TAZ_grouped = DOW_group.groupby('OTAZ')\n",
    "    print(\">>> Now start to process weekday:\", DOW) \n",
    "    \n",
    "    newList = [['Day_of_Week','TAZ_ID','DLAT','DLON','StartTime','EndTime', 'Total_Sec', 'PersonID_1', 'ActivityID_1', 'PersonID_2', 'ActivityID_2','Gender','Student','Educ','Wrkr','Age']]\n",
    "    for TAZ, TAZ_df in TAZ_grouped:\n",
    "        total_cnt = len(TAZ_df)\n",
    "        #if there is more than 2 records then start the comparision of start time and end time \n",
    "        if total_cnt >= 2:\n",
    "            for i in range(0, total_cnt-1):\n",
    "                line01 = TAZ_df.iloc[i]\n",
    "                frTime01 = line01['Start_Time']\n",
    "                toTime01 = line01['End_Time']\n",
    "                for j in range(i+1, total_cnt):\n",
    "                    line02 = TAZ_df.iloc[j]\n",
    "                    frTime02 = line02['Start_Time']\n",
    "                    toTime02 = line02['End_Time']\n",
    "                    #find the max \"from time(start time)\" and min \"to time(end time)\" between two records\n",
    "                    #and calculate their total interaction seconds\n",
    "                    maxfrTime = max(frTime01, frTime02)\n",
    "                    mintoTime = min(toTime01, toTime02)\n",
    "                    durSeconds = int((mintoTime - maxfrTime).total_seconds())\n",
    "                    # if total interaction seconds greater than 0 then extract the trip data into new row \n",
    "                    if durSeconds > 0:\n",
    "                        newRow = [DOW,TAZ,line01['DLAT'],line01['DLON'],maxfrTime.time().strftime(\"%H:%M:%S\"), mintoTime.time().strftime(\"%H:%M:%S\"), durSeconds,\n",
    "                                line01['HHPERSONID'], line01['ACTIVITY_ID'],line02['HHPERSONID'],line02['ACTIVITY_ID'],line01['Gender'],line01['Student'],line01['Educ'],line01['Wrkr'],line01['Age3']]\n",
    "                        newList.append(newRow)\n",
    "                      \n",
    "    #append all the new row into new dataframe  \n",
    "    new_df = pd.DataFrame(newList[1:],columns=newList[0])   \n",
    "    newFileName = 'Interaction_ByDOW_'+str(int(DOW))+ '.csv'\n",
    "   \n",
    "    new_df.to_csv(newFileName, index=False)\n",
    "    print(f\">>> Finish writing the output data {newFileName}\")             \n",
    "\n",
    "print(\"Finished\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = pd.read_csv('Interaction_ByDOW_2.csv',encoding=\"ISO-8859-1\")\n",
    "#test#['Gender'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
